# 23.6 RCU代码

为了巩固前面介绍的内容，接下来看一段使用了RCU的简单代码。上半段是读取被RCU保护的链表 ，下半段代码是替换链表的第一个元素。

![](../.gitbook/assets/image%20%28712%29.png)

数据读取位于rcu\_read\_lock和rcu\_read\_unlock之间，这两个函数几乎不做任何事情。rcu\_read\_lock会设置一个标志位，表明如果发生了定时器中断，请不要执行context switch，因为我正在RCU critical 区域中。所以rcu\_read\_lock会设置一个标志位来阻止定时器中断导致的context switch，中断或许还会发生，但是不会导致context switch。rcu\_read\_unlock取消该标志位。所以这是一个集成在RCU critical区域的计数器。rcu\_read\_lock和rcu\_read\_unlock因为几乎不做任何工作所以极其的快。

其中的while循环会扫描链表，rcu\_dereference函数会插入memory barrier，它首先会从内存中拷贝e，触发一个memory barrier，之后返回指向e的指针。之后我们就可以读取e指针指向的数据内容，并走向下一个链表元素。数据读取部分非常简单。

数据写入部分更复杂点。RCU并不能帮助数据写入者之间避免相互干扰，所以必须有一种方法能确保一次只能有一个数据写入者更新链表。这里我们假设我们将使用普通的spinlock，所以最开始数据写入者获取锁。如果我们要替换链表的第一个元素，我们需要保存先保存链表第一个元素的拷贝，因为最后我们需要释放它，所以有old=head。接下来的代码执行的是之前介绍的内容，首先是分配一个全新的链表元素，之后是设置该链表元素的内容，设置该链表元素的next指针指向旧元素的next指针。之后的rcu\_assign\_pointer函数会设置一个memory barrier，以确保之前的所有写操作都执行完，之后再将head指向新分配的链表元素e。之后就是释放锁。

之后调用synchronize\_rcu确保任何一个可能持有了旧的链表元素的CPU都执行一次context switch，因此这些CPU会放弃指向旧链表元素的指针。最后是释放旧的链表元素。

这里有件事情需要注意，在数据读取代码中，我们可以在循环中查看链表元素，但是我们不能将链表元素返回。例如，我们使用RCU的时候，不能写一个list\_lookup函数来返回链表元素或者返回指向链表元素中数据的指针，也就是嵌入在链表元素中的字符串。我们必须只在RCU critical区域内查看被RCU保护的数据，如果我们写了一个通用的函数返回链表元素，或许我们能要求这个函数的调用者也遵循一些规则，但是函数的调用者还是可能会出发context switch，并且我们在函数的调用者返回之前调用了rcu\_read\_unlock，这是非法的因为现在定时器中断可以迫使context switch。所以使用RCU的确会向数据读取者增加一些之前并不存在的限制。

> 学生提问：这样是不是说，如果我们不可能返回下标是i的元素所包含的内容？
>
> Robert教授：可以返回一个拷贝，如果e-&gt;x是个字符串，那么我们可以返回一个该字符串的拷贝，这是没有问题的。但是如果我们直接返回一个指针指向e-&gt;x，那就违反了RCU规则。实际上返回e中的任何指针都是错误的，因为我们不能在持有指向RCU保护数据的指针时，发生context switch。这里的风格是直接在RCU critical区域使用这些数据。

接下来我将再简短的介绍性能。如果你使用RCU，数据读取非常的快，除了读取数据本身的开销之外就没有别的额外的开销了。如果你的链表有10亿个元素，读取链表本身就要很长的时间，但是这里的时间消耗并不是因为同步（注，也就是类似加锁等操作）引起的。所以你几乎可以认为RCU对于数据读取者来说没有额外的负担。唯一额外的工作就是在rcu\_read\_lock和rcu\_read\_unlock里面设置好不要触发context switch，并且在rcu\_dereference中设置memory barrier，这些可能会消耗几十个CPU cycle，但是相比锁来说代价要小的多。

对于数据写入者，性能会更加的糟糕。首先之前使用锁的时候的所有工作仍然需要做，例如获取锁和释放锁。其次，现在还有了一个可能非常耗时的synchronize\_rcu函数调用。实际上在synchronize\_rcu内部会出让CPU，所以代码在这不会消耗CPU等待，但是它可能会消耗大量时间来等待其他所有的CPU核完成context switch。所以基于数据写入时的多种原因，和数据读取时的工作量，性能提升非常大，如果数据读取区域很短的话，并且数据写入并没有很多，那么数据写入或许会很慢也没关系。所以当人们将RCU应用到内核中时，必须要做一些性能测试来确认使用RCU是否能带来好处，因为这实际取决与工作负载。

> 70:00 - 73:20（不相关的问题，故略过）

你们应该已经看到了RCU并不是广泛通用的，你不能把所有使用spinlock并且性能很差的场景转化成使用  
RCU，并获得更好的性能，主要的原因是RCU完全帮不到写操作，甚至会让写操作更慢，只有当读操作远远多于读操作时才有可能应用RCU。RCU有这样的限制，代码不能在sleep的时候持有指针指向被RCU保护的数据，这会使得一些代码非常奇怪，当一定要sleep的时候，在sleep之后需要重新进入RCU critical区域再次查找之前已经看过的数据，如果这些数据还存在的话。所以RCU使得代码稍微复杂了一些。

另一方面可以直接应用RCU的数据结构在更新时，需要能支持单个committing write操作。你不能在原地更新数据，而是必须创建一个新的元素对象带代替之前的元素对象。所以单链表，树是可以应用RCU的数据结构，但是一些复杂的数据结构不能直接使用RCU。[论文](https://pdos.csail.mit.edu/6.828/2020/readings/rcu-decade-later.pdf)里面提到了一些更复杂的方法，例如sequence lock，可以允许原地更新数据的同时，又不用数据读取者使用锁。但是这些方法要复杂一些，并且能够提升性能的场景也是受限的。

另一个小问题是，并没有明确的时间点表明数据读取者仍然能看到旧的数据，因为如果某些数据读取者在数据写入者替换之前，获取了一个指针指向被RCU保护的数据，数据读取者可能会在较长的时间内持有这个数据。大部分时候这都无所谓，但是论文提到了一些场景人们期望写操作在完成时立即生效，这时读到旧数据会有点让人感到意外。

作为一个独立的话题，你们或许会想对于一个写操作频繁的数据改如何提升性能。RCU只关心读操作频繁的数据，但是这类数据只代表了一种场景。在一些特殊场景中，写操作频繁的数据也可以获取好的性能，但是我还不知道存在类似RCU这样通用的方法能优化写操作频繁的数据。但是仍然存在一些思路可以处理写操作频繁的数据。最有效的方法就是重新构造你的数据结构，这样它就不是共享的。有的时候共享数据完全是没必要的，一旦你发现共享是个问题，你可以让数据不共享。

但是某些时候你又的确需要共享的数据，但是这些共享数据有没有必要被不同的CPU写入。实际上你们已经在lab中见过这样的数据，在locking lab的kalloc部分，你们重构了free list使得每个CPU核都有了一个专属的free list，这实际上就是将一个频繁写入的数据转换成了每个核的半私有数据，大部分时候CPU核不会与其他CPU核冲突，因为它们都有属于自己的free list。唯一的需要查看其他CPU核的free list的场景是自己的free list用光了。这里有很多类似的例子用来处理内核中需要频繁写入的数据，例如Linux中的内存分配，线程调度列表，对于每个CPU核都有一套独立的线程以供线程调取器查看（注，详见11.8，线程对象存储在struct cpu中）。CPU核只有在自己所有工作都完成的时候才会查看其他CPU核的线程调度列表。另一个例子是统计计数，如果你在对某个行为计数，但是计数变化的很频繁，同时很少被读出，你可以重构你的计数器，这样每个CPU核都有个独立的计数器，这样每个CPU核只需要更新属于自己的计数器，当你需要读取时，你只需要通过加锁读出每个CPU核的计数器再加在一起。

这些都是可以让写操作变得更快的方法，因为数据写入者只需要更新当前CPU核的计数器，但是数据读取者现在变得更慢了。但是如果你的计数器需要频繁写入，实际上通常的计数器都需要频繁写入，这将会是一个巨大的收益，将更多的工作转换到数据读取操作上。

这里想说的是，即使我们并没有太讨论，但是的确存在一些技术在某些场合可以帮助提升频繁写入数据的性能。

最后总结一下，论文中介绍的RCU对于Linux来说是一个巨大的成功。它在Linux中各种数据都有使用，实际中需要频繁读取的数据还挺常见的，例如block cache基本上就是被读取。所以一种只提升读性能的技术能够应用的非常广泛。尽管有许多有趣的并发技术，同步（synchronization）技术，RCU还是很神奇，因为它对数据读取者完全去除了锁和数据写入（注，例如读写锁时的计数），所以相比读写锁，RCU是一个很大的突破。RCU能工作的核心思想是为资源释放（Garbage Collection）增加了grace period，直到所有的数据读取者都确保用完了数据。所以尽管是一种同步技术，也可以将其看做是一种特殊的GC技术。

> 学生提问：为什么数据读取者可以读到旧数据呢？因为在RCU critical区域里，你看到的就是实际存在的数据。
>
> Robert教授：通常来说这不是个问题。通常来说，你写代码，将1赋值给x，之后print ”done“。

![](../.gitbook/assets/image%20%28710%29.png)

> 在print之后，如果有人读取x，可能会看到你在将1赋值给x之前的数值，这里或许有些出乎意料。而RCU允许这种情况发生，如果我们在使用RCU时，并将数据赋值改成list\_replace，将包含1的元素的内容改成2。

![](../.gitbook/assets/image%20%28713%29.png)

> 在函数结束后，我们print ”done“。如果一些其他的数据读取者在查看链表，它们或许刚刚看到了持有1的链表元素，之后它们过了一会才实际的读取链表元素，它们会看到旧的数值1（注，因为RCU是用替换的方式实现更新，数据读取者可能读到了旧元素的指针，里面一直包含的是旧的数值）。所以这就有点奇怪了，就算添加memory barrier也不能避免这种情况。不过实际上大部分场景下这也没关系，因为这里数据的读写者是并发的，通常来说如果两件事情是并发执行的，你是不会认为它们的执行顺序是确定的。
>
> 但是论文的确举了个例子说读到旧数据是有关系的，并且会触发一个实际的问题，尽管我并不太理解为什么会有问题。
>
> 学生提问：RCU之所以被称为RCU，是因为它的基本实现对吧？
>
> Robert教授：Read-Copy-Update，是的我认为是因为它的基本实现，它不是在原地修改数据，你穿件了一个拷贝再来更新。
>
> 学生提问：在介绍读写锁时，我们讨论了为了实现缓存一致需要O\(n^2\)时间。对于spinlock这是不是也是个问题，为什么我们在之前在介绍spinlock的时候没有讨论这个问题，是因为spinlock有什么特殊的操作解决了这个问题吗？
>
> Robert教授：并没有，锁的代价都很高。如果没有竞争的话，例如XV6中的标准spinlock非常的快。但是如果有大量的CPU核在相同的时候要获取相同的锁就会特别的慢。存在一些其他的锁，在更高负载的时候性能更好，但是在更低负载的时候性能反而更差。这里很难有完美的方案。
>
> 学生提问：或许并不相关，可能存在不同操作系统之间的锁吗？
>
> Robert教授：在分布是系统中，存在一种锁可以存在于多个计算机之间。一个场景是分布式数据库，你将数据分发给多个计算机，但是如果你想要执行一个transaction，并使用分布在多个计算机上的数据，你将会需要从多个计算机上收集锁。另一个场景是，有一些系统会尝试在独立的计算机之间模拟共享内存，比如说一个计算机使用了另一个计算机的内存，背后需要有一些工具能够使得计算机之间能交互并请求内存，然后可以在一个集群的计算机上运行一些现有的并行程序，而不是在一个大的多核计算机上，因为这样成本会更低。这时需要对spinlock或者任何你使用的锁做一些处理，人们发明了各种技术来使得锁能很好的工作，这些技术与我们介绍的技术就不太一样了。尽管避免性能损失的压力会更大。

