# 10.1 为什么要使用锁？

（00:00 - 01:33）是上一个lab的抽查问答，与内容无关故跳过。

今天的课程的内容是锁。你们或许在其他的课程中已经学习过锁，这节课偏向于理论介绍，并且或许会与其他课程中有关锁的内容有些重合，不过这节课更关注内核和操作系统中的锁。

首先，我们来回顾一下，为什么我们需要锁？故事要从应用程序想要使用多个CPU核开始，使用多个CPU核可以带来性能的提升。如果一个应用程序运行在多个CPU核上，并且执行了系统调用，那么内核需要能够处理并行的系统调用。如果系统调用并行的在多个CPU核上运行，那么它们可能会并行的访问共享的数据结构。到目前为止，你们也看到了XV6有很多共享的数据结构，例如proc，ticks，之后我们还会看到buffer cache等等。如果并行的访问数据结构，例如一个核在读取数据，另一个核在写入数据，我们需要使用锁来协调对于共享数据的更新，这样数据可以保持一致性。所以，我们需要锁来控制并确保数据共享是正确的。

但是现在的处境有些令人失望，因为我们想要通过并行来获得高性能，我们想要并行的在不同的CPU核上执行系统调用，但是如果这些系统调用使用过了共享的数据，我们又需要使用锁，而锁会将这些系统调用串行执行，所以最后锁又限制了性能。

![](../.gitbook/assets/image%20%28450%29.png)

所以现在我们处于一个矛盾的处境，出于正确性，我们需要使用锁，但是考虑到性能，锁又是极不好的。这就是现实，我们接下来会看看如何改善这个处境。

以上是一个整体的介绍，但是回到最开始，为什么应用程序一定要使用多个CPU核来提升性能呢？这个实际上与过去几十年技术的发展有关。下面这张经典的图可以解释为什么。

![](../.gitbook/assets/image%20%28448%29.png)

这张图有点复杂，X轴是时间，Y轴是单位，具体意义取决于特定的曲线。这张图中的核心点是，从2000年开始，CPU的时钟频率就没有再增加过了（绿线）。这样的结果是，CPU的单线程性能也达到了一个极限并且没有再增加过（蓝线）。但是另一方面，CPU中的晶体管数量在持续的增加 （深红色线）。所以现在不能通过使用单核来让代码运行的更快，要想运行的更快，唯一的选择就是使用多核。所以从2000年开始，处理器上核的数量开始在增加。所以现在如果一个应用程序想要提升性能，它不能只依赖单核，而是必须要依赖于多核。这也意味着，如果应用程序与内核交互的较为紧密，那么操作系统也需要高效的在多个CPU核上运行。这就是我们对内核并行在多个CPU核上运行感兴趣的直接原因。你们可能之前已经看过上面这张图，但我们这里回顾一下背景知识也是极好的。

那为什么要使用锁呢？前面我们已经提到了，是为了确保正确性。当一份共享数据同时被读写时，如果没有锁的话，可能会出现race condition，进而导致程序出错。race condition还是比较讨厌的，我们先来看看什么是race condition。我们在XV6中创建一个race condition，然后看看它的表象是什么。

在kalloc.c文件中的kfree函数中，会将释放的page保存于freelist中。

![](../.gitbook/assets/image%20%28483%29.png)

XV6有一个非常简单的数据结构会将所有的free page保存于列表中。这样当kalloc函数需要一个内存page时，它可以从freelist中获取。从函数中可以看出，这里有一个锁kmem。在上锁的区间内程序更新了freelist。这里我们将锁的acquire和release注释上，这样原来在上锁区间内的代码就不再是原子执行的了。

![](../.gitbook/assets/image%20%28476%29.png)

之后运行make qemu重新编译XV6，

![](../.gitbook/assets/image%20%28465%29.png)

我们可以看到XV6已经运行起来，并且我们应该已经运行了一些对于kfree的调用，看起来一切运行都正常啊。

接下来运行一下usertest，究竟能不能成功呢？有人想猜一下吗？

> 学生回答：如果发生了race condition就会丢失一些内存page，如果没有发生就能成功。

是的，race condition不一定会发生，让我们来运行一下usertest，看看究竟会发生什么。我这里通过qemu模拟了3个CPU核，这3个核是并行运行的。这里的测试有点慢，我们先切到课件中，待会回来看一下。但是如刚刚那位同学指出的，race condition不一定会发生，因为当每一个核在每一次调用kfree函数时，对于freelist的更新都是原子操作，这就像有锁一样，这个时候没有问题。有问题的是，当两个处理器，两个线程同时调用kfree，并且对于freelist交错执行时。

我们来看一下usertest运行的结果，可以看到已经有panic了。所以的确有一些race condition触发了panic。但是还有一些其他的race condition，如前面的同学提到的，可能会导致丢失一些内存page，这样的话，usertest运行不会有问题。

![](../.gitbook/assets/image%20%28486%29.png)

所以race condition可以有不同的表现形式，它可能发生，也可能不发生。但是在这里的usertests中，很明显发生了什么。让我们来分析一下哪里出错了。

首先你们在脑海里应该有多个CPU核在运行，比如说CPU0在运行指令，CPU1也在运行指令，这两个CPU核都连接到同一个内存上。在前面的代码中，数据freelist位于内存中，它里面记录了2个内存page。假设两个CPU核都在大概相近的时间调用kfree。

![](../.gitbook/assets/image%20%28467%29.png)

kfree函数接收一个物理地址pa作为参数，freelist是个单链表，kfree中将pa作为freelist的新的head，并更新freelist指向pa。当两个CPU都调用kfree时，CPU0想要释放一个page，CPU1也想要释放一个page，现在这两个page都需要加到freelist中。

![](../.gitbook/assets/image%20%28490%29.png)

kfree中首先将对应page的变量r指向了当前的freelist（也就是单链表当前的head指针）。我们假设CPU0先运行，那么CPU0会将它的变量r的next指向当前的freelist。如果CPU1在同一时间运行，它可能在CPU0运行第二条指令（kmem.freelist = r）之前运行代码。所以它也会完成相同的事情，它会将自己的变量r的next指向当前的freelist。现在两个物理page对应的变量r都指向了同一个freelist。

![](../.gitbook/assets/image%20%28445%29.png)

接下来，剩下的代码也会并行的执行（kmem.freelist = r）这行代码会更新freelist为r。因为我们这里只有一个内存，所以总是有一个CPU会先执行，我们假设CPU0先执行，那么freelist会等于CPU0的变量r。之后CPU1再执行，它又会将freelist更新为CPU1的变量r。这样的结果是，我们丢失了CPU0对应的page。CPU0想要释放的内存page最终没有出现在freelist数据中。

这是一种具体的坏的结果，当然可能会有更多坏的结果因为可能会有更多的CPU，其中一个CPU可能会短暂的发现freelist等于CPU对应的变量r，并且使用这个page，但是之后很快freelist又被CPU1更新了。所以，拥有越多的CPU，我们就可能看到比丢失page更奇怪的现象。

在代码中，用来解决这里的问题的最常见方法就是使用锁。

